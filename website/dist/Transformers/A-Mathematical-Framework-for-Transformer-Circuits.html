<!DOCTYPE html>
<html lang="en"><head><title>A Mathematical Framework for Transformer Circuits</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="A Mathematical Framework for Transformer Circuits"/><meta property="og:description" content="What is Mechanistic Interpretability? Attempting to reverse engineer the detailed computation performed by Neural Networks (and now Transformers!) Describe the working of these Networks in simpler Mathematical Functions."/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="What is Mechanistic Interpretability? Attempting to reverse engineer the detailed computation performed by Neural Networks (and now Transformers!) Describe the working of these Networks in simpler Mathematical Functions."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Transformers/A-Mathematical-Framework-for-Transformer-Circuits"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title"><a href="..">ü™¥ Quartz 4.0</a></h1><div class="spacer mobile-only"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div><div class="explorer desktop-only"><button type="button" id="explorer" data-behavior="collapse" data-collapsed="collapsed" data-savestate="true" data-tree="[{&quot;path&quot;:&quot;Posts&quot;,&quot;collapsed&quot;:true},{&quot;path&quot;:&quot;Posts/images&quot;,&quot;collapsed&quot;:true},{&quot;path&quot;:&quot;Transformers&quot;,&quot;collapsed&quot;:true}]"><h1>Explorer</h1><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div data-folderpath="Posts"><button class="folder-button"><span class="folder-title">Posts</span></button></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Posts"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div data-folderpath="Posts/images"><button class="folder-button"><span class="folder-title">images</span></button></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Posts/images"><li><a href="../Posts/images/Pasted" data-for="Posts/images/Pasted">Pasted</a></li></ul></div></li><li><a href="../Posts/About" data-for="Posts/About">About</a></li><li><a href="../Posts/Frontend-Development" data-for="Posts/Frontend-Development">Frontend Development</a></li><li><a href="../Posts/Github-Pages" data-for="Posts/Github-Pages">Github Pages</a></li><li><a href="../Posts/My-Portfolio-Page" data-for="Posts/My-Portfolio-Page">My Portfolio Page</a></li><li><a href="../Posts/New-Website" data-for="Posts/New-Website">New Website</a></li><li><a href="../Posts/Welcome" data-for="Posts/Welcome">Welcome</a></li></ul></div></li><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div data-folderpath="Transformers"><button class="folder-button"><span class="folder-title">Transformers</span></button></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Transformers"><li><a href="../Transformers/A-Mathematical-Framework-for-Transformer-Circuits" data-for="Transformers/A-Mathematical-Framework-for-Transformer-Circuits">A Mathematical Framework for Transformer Circuits</a></li><li><a href="../Transformers/Eigenvalue" data-for="Transformers/Eigenvalue">Eigenvalue</a></li><li><a href="../Transformers/Kronecker-Products" data-for="Transformers/Kronecker-Products">Kronecker Products</a></li><li><a href="../Transformers/MLP" data-for="Transformers/MLP">MLP</a></li><li><a href="../Transformers/Perceptron" data-for="Transformers/Perceptron">Perceptron</a></li><li><a href="../Transformers/Transformer" data-for="Transformers/Transformer">Transformer</a></li></ul></div></li></ul></div></li><li id="explorer-end"></li></ul></div></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Home</a><p> ‚ùØ </p></div><div class="breadcrumb-element"><a href="../Transformers/">Transformers</a><p> ‚ùØ </p></div><div class="breadcrumb-element"><a href>A Mathematical Framework for Transformer Circuits</a></div></nav><h1 class="article-title">A Mathematical Framework for Transformer Circuits</h1><p class="content-meta">Feb 29, 2024, 4 min read</p></div></div><article class="popover-hint"><h2 id="what-is-mechanistic-interpretability">What is <em><em>Mechanistic Interpretability</em>?</em><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#what-is-mechanistic-interpretability" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<hr/>
<ul>
<li>Attempting to reverse engineer the detailed computation performed by Neural Networks (and now <em>Transformers</em>!)</li>
<li>Describe the working of these Networks in simpler Mathematical Functions.</li>
</ul>
<hr/>
<h2 id="what-is-our-goal">What is our Goal?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#what-is-our-goal" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<hr/>
<ul>
<li>Discover Algorithmic Patterns, Motifs, Frameworks which can be applied to larger models</li>
</ul>
<hr/>
<h2 id="simplifying-assumptions-made">Simplifying assumptions made<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#simplifying-assumptions-made" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<hr/>
<ol>
<li>No <a href="../Transformers/MLP" class="internal alias" data-slug="Transformers/MLP">MLP</a> (Multi Layer <a href="../Transformers/Perceptron" class="internal alias" data-slug="Transformers/Perceptron">Perceptron</a> / Fully Connected Neural Network) Layers</li>
<li>No Biases in Weights (If at all required we can modify weights creating an additional dimension that is always one but it doesn‚Äôt affect any insights discovered in the paper)</li>
<li>No <em>Layer Norm</em></li>
</ol>
<h3 id="non-standard-but-equivalent-representation-of-the-transformer">Non Standard (but equivalent) Representation of the <a href="../Transformers/Transformer" class="internal alias" data-slug="Transformers/Transformer">Transformer</a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#non-standard-but-equivalent-representation-of-the-transformer" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../Transformers/images/Pasted-image-20240220140453.png" alt="Img"/></p>
<h3 id="what-is-the-residual-stream">What is the ‚ÄúResidual Stream‚Äù?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#what-is-the-residual-stream" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../Transformers/images/Pasted-image-20240220141011.png" alt="Img"/></p>
<hr/>
<h2 id="zero-layer-attention-models-bigram-statistics">Zero Layer <em>Attention</em> models <em>Bigram Statistics</em><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#zero-layer-attention-models-bigram-statistics" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<hr/>
<blockquote class="callout bug" data-callout="bug">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Due to the absence of the Attention Mechanism, only the last produced token can influence the production of the next one. </p></div>
                  
                </div>
<ul>
<li>
<p>This should mean that the model would try to learn the Bigram Statistics table of the Dataset and use it to produce tokens.</p>
</li>
<li>
<p>This also means there is no ‚ÄúIn-Context Learning‚Äù as the context is only the last produced token.</p>
</li>
</ul>
</blockquote>
<h6 id><img src="../Transformers/images/Pasted-image-20240220141338.png" alt="Img"/></h6>
<hr/>
<h2 id="one-layer-attention-transformer-is-an-ensemble-of-bigram-statistics-and-skip-trigrams">One Layer Attention Transformer is an ensemble of Bigram Statistics and ‚ÄúSkip-Trigrams‚Äù<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#one-layer-attention-transformer-is-an-ensemble-of-bigram-statistics-and-skip-trigrams" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="kronecker-products"><a href="../Transformers/Kronecker-Products" class="internal alias" data-slug="Transformers/Kronecker-Products">Kronecker Products</a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#kronecker-products" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>During the forward propogation of Transformers, we frequently do two ‚Äútypes‚Äù of multiplications</p>
<ol>
<li>Multiplying each vector in a Matrix with another Matrix (Eg: to convert tokens to queries / keys)</li>
<li>Weighted average of the Vectors in a Matrix (Eg: to get the final representation of a token by the weighted average of all the other tokens, assigning weights after attending to them)</li>
</ol>
<p>We can represent this more intuitively using <a href="../Transformers/Kronecker-Products" class="internal alias" data-slug="Transformers/Kronecker-Products">Kronecker Products</a>!</p>
<h3 id="how-is-the-next-token-produced">How is the next <em>token</em> produced?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#how-is-the-next-token-produced" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Let‚Äôs see how the One Layer Transformer produces the next token by updating the representation of the Last Produced Token.</p>
<p>To do this, it considers the entire context available to the model.</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220141804.png" alt="Img"/></p>
</blockquote>
<h3 id="path-expansion-trick">Path Expansion Trick<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#path-expansion-trick" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220144023.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220151037.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220142124.png" alt="Img"/></p>
</blockquote>
<h3 id="the-qk--ov-circuits">The QK &amp; OV Circuits<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-qk--ov-circuits" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220150657.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220150926.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220150750.png" alt="Img"/></p>
</blockquote>
<blockquote class="callout quote" data-callout="quote">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Splitting the computation into two seperable operations </p></div>
                  
                </div>
<p><img src="../Transformers/images/Pasted-image-20240220145956.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220142315.png" alt="Img"/></p>
</blockquote>
<ul>
<li>The Attention Pattern depends on the Source and Destination Token, but once a Destination token has decided how much to ‚ÄúAttend‚Äù to a source token, the effect on the output is a function only of that source token.</li>
<li>If multiple destination tokens attend to the same source token in the same amount, the source token will have the same effect on the Logits for the predicted output token.</li>
</ul>
<h2 id="eigenvalue-analysis"><a href="../Transformers/Eigenvalue" class="internal alias" data-slug="Transformers/Eigenvalue">Eigenvalue</a> Analysis<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#eigenvalue-analysis" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h4 id="can-we-detect-whether-an-attention-head-is-copying">Can we ‚Äúdetect‚Äù whether an attention head is ‚ÄúCopying‚Äù?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#can-we-detect-whether-an-attention-head-is-copying" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>According to the authors, though they don‚Äôt provide a formal proof for this, the presence of large, positive <a href="#Eigenvalue%20Analysis" class="internal alias">Eigenvalues</a> in the OV Circuit is indicative of ‚ÄúCopying‚Äù.</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220151625.png" alt="Img"/></p>
</blockquote>
<p>If we plot a histogram of the Attention Heads with the percentage of ‚ÄúCopying‚Äù Attention Heads on the X-Axis:</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220151642.png" alt="Img"/></p>
</blockquote>
<h2 id="two-layer-attention-transformer-are-more-expressive">Two Layer Attention Transformer are more expressive!<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#two-layer-attention-transformer-are-more-expressive" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="how-do-they-vary-from-one-layer-only-models">How do they vary from One Layer only models?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#how-do-they-vary-from-one-layer-only-models" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>The residual stream is a communication channel. Every Attention head reads in subspaces of the Residual Streams determined by Wq, Wk and Wv and then writes to some subspac determined by Wo.</p>
<p>When Attention Heads compose, there are three options:</p>
<ul>
<li><em>Q-Composition</em>:¬†Wq reads in a subspace affected by a previous head.</li>
<li><em>K-Composition</em>:¬†Wk¬†reads in a subspace affected by a previous head.</li>
<li><em>V-Composition</em>:¬†Wv‚Äã¬†reads in a subspace affected by a previous head.</li>
</ul>
<p>Here, the Q &amp; K composition affect the attention pattern, allowing the second Attention Layer to express much more complex patterns while deciding which token to give attention to.</p>
<p>V composition on the other hand affects what information an Attention Head ‚Äúmoves‚Äù when it attends to a given position.</p>
<h3 id="path-expansion-of-the-o-v-circuit">Path Expansion of the O-V circuit<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#path-expansion-of-the-o-v-circuit" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Freezing the Attention Layers as in the 1L case, we see that V-Composition affects the final output.</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220152547.png" alt="Img"/></p>
</blockquote>
<h3 id="path-expansion-of-the-attention-scores-of-the-qk-circuit">Path Expansion of the Attention Scores of the QK Circuit<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#path-expansion-of-the-attention-scores-of-the-qk-circuit" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>We have written the Attention Pattern in this form earlier in the 1L case.</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220152840.png" alt="Img"/></p>
</blockquote>
<p>We shall attempt to do the same again. But, Cqk takes a much more complex form indicating that the 2nd Layer can implement very complex attention patterns.</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220152759.png" alt="Img"/></p>
</blockquote>
<h3 id="induction-heads">Induction Heads<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#induction-heads" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Let us see an example of how the <em>Attention Heads</em> in a 2L <a href="../Transformers/Transformer" class="internal alias" data-slug="Transformers/Transformer">Transformer</a> work.</p>
<show the demo on website>
<p>The Aqua colored <em>Induction Heads</em> often attend back to the previous instances of the token which will come next!</p>
<p>We call these kind of Attention Heads ‚ÄúInduction Heads‚Äù!</p>
<blockquote class="callout quote" data-callout="quote">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Induction Heads </p></div>
                  
                </div>
<p>Induction heads search over the context for previous examples of the present token.¬†If they don‚Äôt find it, they attend to the first token (in our case, a special token placed at the start), and do nothing. But if they do find it, they then look at the¬†next¬†token and copy it. This allows them to repeat previous sequences of tokens, both exactly and approximately.</p>
<blockquote>
<p><img src="../Transformers/images/Pasted-image-20240220153446.png" alt="Img"/></p>
</blockquote>
</blockquote>
<h3 id="how-do-induction-heads-work">How do <em>Induction Heads</em> Work?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#how-do-induction-heads-work" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../Transformers/images/Pasted-image-20240220153517.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220153517.png" alt="Img"/></p>
<h3 id="the-simplest-way-an-induction-head-could-be-made">The Simplest way an Induction Head could be made<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-simplest-way-an-induction-head-could-be-made" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../images/Pasted-image-20240220154039.png" alt="Pasted image 20240220154039.png"/></p>
<h3 id="term-importance-analysis">Term Importance Analysis<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#term-importance-analysis" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><img src="../Transformers/images/Pasted-image-20240220154109.png" alt="Img"/></p>
<p><img src="../Transformers/images/Pasted-image-20240220154114.png" alt="Img"/></p>
<h3 id="interesting-visualizations-of-induction-heads">Interesting Visualizations of Induction Heads<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#interesting-visualizations-of-induction-heads" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>&lt;Show the Youtube Video?/>
<img src="../Transformers/images/Pasted-image-20240220123903.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220123242.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220123301.png" alt="Img"/>
<img src="../Transformers/images/Pasted-image-20240220123146.png" alt="Img"/></p></show></article></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[]}"></div><svg version="1.1" id="global-graph-icon" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
	s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
	c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
	C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
	c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
	v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
	s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
	C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
	S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
	s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
	s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[]}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#what-is-mechanistic-interpretability" data-for="what-is-mechanistic-interpretability">What is Mechanistic Interpretability?</a></li><li class="depth-0"><a href="#what-is-our-goal" data-for="what-is-our-goal">What is our Goal?</a></li><li class="depth-0"><a href="#simplifying-assumptions-made" data-for="simplifying-assumptions-made">Simplifying assumptions made</a></li><li class="depth-1"><a href="#non-standard-but-equivalent-representation-of-the-transformer" data-for="non-standard-but-equivalent-representation-of-the-transformer">Non Standard (but equivalent) Representation of the Transformer</a></li><li class="depth-1"><a href="#what-is-the-residual-stream" data-for="what-is-the-residual-stream">What is the ‚ÄúResidual Stream‚Äù?</a></li><li class="depth-0"><a href="#zero-layer-attention-models-bigram-statistics" data-for="zero-layer-attention-models-bigram-statistics">Zero Layer Attention models Bigram Statistics</a></li><li class="depth-0"><a href="#one-layer-attention-transformer-is-an-ensemble-of-bigram-statistics-and-skip-trigrams" data-for="one-layer-attention-transformer-is-an-ensemble-of-bigram-statistics-and-skip-trigrams">One Layer Attention Transformer is an ensemble of Bigram Statistics and ‚ÄúSkip-Trigrams‚Äù</a></li><li class="depth-1"><a href="#kronecker-products" data-for="kronecker-products">Kronecker Products</a></li><li class="depth-1"><a href="#how-is-the-next-token-produced" data-for="how-is-the-next-token-produced">How is the next token produced?</a></li><li class="depth-1"><a href="#path-expansion-trick" data-for="path-expansion-trick">Path Expansion Trick</a></li><li class="depth-1"><a href="#the-qk--ov-circuits" data-for="the-qk--ov-circuits">The QK &amp; OV Circuits</a></li><li class="depth-0"><a href="#eigenvalue-analysis" data-for="eigenvalue-analysis">Eigenvalue Analysis</a></li><li class="depth-0"><a href="#two-layer-attention-transformer-are-more-expressive" data-for="two-layer-attention-transformer-are-more-expressive">Two Layer Attention Transformer are more expressive!</a></li><li class="depth-1"><a href="#how-do-they-vary-from-one-layer-only-models" data-for="how-do-they-vary-from-one-layer-only-models">How do they vary from One Layer only models?</a></li><li class="depth-1"><a href="#path-expansion-of-the-o-v-circuit" data-for="path-expansion-of-the-o-v-circuit">Path Expansion of the O-V circuit</a></li><li class="depth-1"><a href="#path-expansion-of-the-attention-scores-of-the-qk-circuit" data-for="path-expansion-of-the-attention-scores-of-the-qk-circuit">Path Expansion of the Attention Scores of the QK Circuit</a></li><li class="depth-1"><a href="#induction-heads" data-for="induction-heads">Induction Heads</a></li><li class="depth-1"><a href="#how-do-induction-heads-work" data-for="how-do-induction-heads-work">How do Induction Heads Work?</a></li><li class="depth-1"><a href="#the-simplest-way-an-induction-head-could-be-made" data-for="the-simplest-way-an-induction-head-could-be-made">The Simplest way an Induction Head could be made</a></li><li class="depth-1"><a href="#term-importance-analysis" data-for="term-importance-analysis">Term Importance Analysis</a></li><li class="depth-1"><a href="#interesting-visualizations-of-induction-heads" data-for="interesting-visualizations-of-induction-heads">Interesting Visualizations of Induction Heads</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="../" class="internal">_index</a></li><li><a href="../" class="internal">index</a></li></ul></div></div></div><footer class><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> ¬© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></body><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="../postscript.js" type="module"></script></html>